Harry volek initially trained the model without properly extracting silence and speech segments in the audio files.

A VAD method based on the pyAudioAnalysis is adopted here 

Model was trained by this new method analyzing the train and validation loss. However, the model did not converge using this method. 

On July 22nd 2020, a face palm moment happened. The speech signal was not normalized before feature extraction. This is a horrible mistake!

After training with the normalized speech features, I realized that model.eval() was not being used during the validation pass

Incorporating the model.eval() was done and training started. 

Model seems to have plateaued at 'Epoch 20/950.. Train loss: 9.148.. Test loss: 12.188.. ' even though the model training resumed till the 25th epoch. The speech checkpoint file is  
'./speech_id_checkpoint_norm/ckpt_epoch_20_batch_id_302.pth'

A learning rate of exactly half the original i.e 0.005 is applied to further reduce the test loss. Also the checkpoint saving interval is halved i.e. 10 epochs (from 20)

After 10 epochs, the loss reduced to 'Epoch 10/950.. Train loss: 7.600.. Test loss: 5.059..'

A further reduction in learning rate by 20% is done i.e. 0.001(from 0.005)

The plan is to include voxceleb2 dataset as well. Plan is to train with the ensemble of vox1 and vox2

vox2 is in .m4a format. Hence to normalize .m4a files, decoding to .wav is required via the command 
'avconv -i input.m4a output.wav'
The resultant .wav files are 16-bit PCM encoded and hence the normalization factor is 2**15

